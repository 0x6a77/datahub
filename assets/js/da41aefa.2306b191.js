(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[2768],{3905:function(e,t,a){"use strict";a.d(t,{Zo:function(){return l},kt:function(){return m}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var c=n.createContext({}),u=function(e){var t=n.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},l=function(e){var t=u(e.components);return n.createElement(c.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,c=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),p=u(a),m=r,h=p["".concat(c,".").concat(m)]||p[m]||d[m]||i;return a?n.createElement(h,o(o({ref:t},l),{},{components:a})):n.createElement(h,o({ref:t},l))}));function m(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=p;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var u=2;u<i;u++)o[u]=a[u];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},114:function(e,t,a){"use strict";a.r(t),a.d(t,{frontMatter:function(){return o},metadata:function(){return s},toc:function(){return c},default:function(){return l}});var n=a(2122),r=a(9756),i=(a(7294),a(3905)),o={title:"Metadata Ingestion Architecture",sidebar_label:"Metadata Ingestion Architecture",slug:"/architecture/metadata-ingestion",custom_edit_url:"https://github.com/linkedin/datahub/blob/master/docs/architecture/metadata-ingestion.md"},s={unversionedId:"docs/architecture/metadata-ingestion",id:"docs/architecture/metadata-ingestion",isDocsHomePage:!1,title:"Metadata Ingestion Architecture",description:"DataHub supports an extremely flexible ingestion architecture that can support push, pull, asynchronous and synchronous models.",source:"@site/genDocs/docs/architecture/metadata-ingestion.md",sourceDirName:"docs/architecture",slug:"/architecture/metadata-ingestion",permalink:"/docs/architecture/metadata-ingestion",editUrl:"https://github.com/linkedin/datahub/blob/master/docs/architecture/metadata-ingestion.md",version:"current",sidebar_label:"Metadata Ingestion Architecture",frontMatter:{title:"Metadata Ingestion Architecture",sidebar_label:"Metadata Ingestion Architecture",slug:"/architecture/metadata-ingestion",custom_edit_url:"https://github.com/linkedin/datahub/blob/master/docs/architecture/metadata-ingestion.md"},sidebar:"overviewSidebar",previous:{title:"DataHub Architecture Overview",permalink:"/docs/architecture/architecture"},next:{title:"DataHub Serving Architecture",permalink:"/docs/architecture/metadata-serving"}},c=[{value:"MCE: The Center Piece",id:"mce-the-center-piece",children:[]},{value:"Pull-based Integration",id:"pull-based-integration",children:[]},{value:"Push-based Integration",id:"push-based-integration",children:[]},{value:"Internal Components",id:"internal-components",children:[{value:"Applying MCE-s to DataHub Service Tier (mce-consumer)",id:"applying-mce-s-to-datahub-service-tier-mce-consumer",children:[]}]}],u={toc:c};function l(e){var t=e.components,o=(0,r.Z)(e,["components"]);return(0,i.kt)("wrapper",(0,n.Z)({},u,o,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"DataHub supports an extremely flexible ingestion architecture that can support push, pull, asynchronous and synchronous models.\nThe figure below describes all the options possible for connecting your favorite system to DataHub.\n",(0,i.kt)("img",{alt:"Ingestion Architecture",src:a(4429).Z})),(0,i.kt)("h2",{id:"mce-the-center-piece"},"MCE: The Center Piece"),(0,i.kt)("p",null,"The center piece for ingestion is the ",(0,i.kt)("a",{parentName:"p",href:"/docs/what/mxe#metadata-change-event-mce"},"Metadata Change Event (MCE)")," which represents a metadata change that is being communicated by an upstream system.\nMCE-s can be sent over Kafka, for highly scalable async publishing from source systems. They can also be sent directly to the HTTP endpoint exposed by the DataHub service tier to get synchronous success / failure responses. "),(0,i.kt)("h2",{id:"pull-based-integration"},"Pull-based Integration"),(0,i.kt)("p",null,"DataHub ships with a Python based ",(0,i.kt)("a",{parentName:"p",href:"/docs/metadata-ingestion"},"metadata-ingestion system")," that can connect to different sources to pull metadata from them. This metadata is then pushed via Kafka or HTTP to the DataHub storage tier. Metadata ingestion pipelines can be ",(0,i.kt)("a",{parentName:"p",href:"/docs/metadata-ingestion#lineage-with-airflow"},"integrated with Airflow")," to set up scheduled ingestion or capture lineage. If you don't find a source already supported, it is very easy to ",(0,i.kt)("a",{parentName:"p",href:"/docs/metadata-ingestion#contributing"},"write your own"),"."),(0,i.kt)("h2",{id:"push-based-integration"},"Push-based Integration"),(0,i.kt)("p",null,"As long as you can emit a ",(0,i.kt)("a",{parentName:"p",href:"/docs/what/mxe#metadata-change-event-mce"},"Metadata Change Event (MCE)")," event to Kafka or make a REST call over HTTP, you can integrate any system with DataHub. For convenience, DataHub also provides simple ",(0,i.kt)("a",{parentName:"p",href:"/docs/metadata-ingestion#using-as-a-library"},"Python emitters")," for you to integrate into your systems to emit metadata changes (MCE-s) at the point of origin."),(0,i.kt)("h2",{id:"internal-components"},"Internal Components"),(0,i.kt)("h3",{id:"applying-mce-s-to-datahub-service-tier-mce-consumer"},"Applying MCE-s to DataHub Service Tier (mce-consumer)"),(0,i.kt)("p",null,"DataHub comes with a Kafka Streams based job, ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/linkedin/datahub/blob/master/metadata-jobs/mce-consumer-job"},"mce-consumer-job"),", which consumes the MCE-s and converts them into the ",(0,i.kt)("a",{parentName:"p",href:"https://linkedin.github.io/rest.li/how_data_is_represented_in_memory#the-data-template-layer"},"equivalent Pegasus format")," and sends it to the DataHub Service Tier (datahub-gms) using the ",(0,i.kt)("inlineCode",{parentName:"p"},"/ingest")," endpoint. "))}l.isMDXComponent=!0},4429:function(e,t,a){"use strict";t.Z=a.p+"assets/images/ingestion-architecture-cd631d7c4a648ceb82908ce25b9f93b9.png"}}]);